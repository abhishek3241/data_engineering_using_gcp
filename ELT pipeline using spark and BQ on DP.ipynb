{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1 : Create DP Workflow Template\n",
    "Step2 : Configure active DP cluster(can alos configure new cluster)\n",
    "Step3 : Add spark SQL or Pyspark Jobs to DP Workflow templates with Dependencies\n",
    "Step4 : Run and Validate the DP Workflow Template\n",
    "\n",
    "All the above steps can be executed using gcloud commands\n",
    "\n",
    "Make sure to copy all the scripts and relevants files to GCS.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set dataproc/region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc workflow-templates list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete existing WF templates:\n",
    "\n",
    "!gcloud dataproc workflow-templates delete template_name --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new WF:\n",
    "!gcloud dataproc workflow-templates create template_name\n",
    "!gcloud dataproc workflow-templates list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now configure your existing Dataproc cluster with your workflow:\n",
    "\n",
    "!gcloud dataproc workflow-templates set-cluster-selector \\\n",
    "    wf_name \\\n",
    "        --cluster-labels goog-dataproc-cluster-name = cluster_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#now run the commands to add jobs to the workflow(spark-sql is the type of job):\n",
    "\n",
    "#cleanup job\n",
    "\n",
    "gcloud dataproc workflow-templates add-job spark-sql \\\n",
    "    --step-id = job-cleanup \\\n",
    "    --file = gs://script_path\n",
    "    --workflow-template= template_name\n",
    "\n",
    "\n",
    "#File Format Convertor with Dependencies on cleanup:\n",
    "\n",
    "gcloud dataproc workflow-templates add-job spark-sql \\\n",
    "    --step-id = job-convert-orders \\\n",
    "    --file = gs://script_path\n",
    "    --params = bucket_name = gs://bucket_name, table_name =orders \\\n",
    "    --workflow-template= template_name \\\n",
    "    --start-after = job-cleanup\n",
    "\n",
    "gcloud dataproc workflow-templates add-job spark-sql \\\n",
    "    --step-id = job-convert-order-items \\\n",
    "    --file = gs://script_path\n",
    "    --params = bucket_name = gs://bucket_name, table_name =order_items \\\n",
    "    --workflow-template= template_name \\\n",
    "    --start-after = job-cleanup\n",
    "\n",
    "#Job to compute  daily revenue:\n",
    "gcloud dataproc workflow-templates add-job spark-sql \\\n",
    "    --step-id = job-daily-product-revenue \\\n",
    "    --file = gs://script_path\n",
    "    --params = bucket_name = gs://bucket_name \\\n",
    "    --workflow-template= template_name \\\n",
    "    --start-after = job-convert-orders,job-convert-order-items\n",
    "\n",
    "#job to read daily revenue data from gcs and write to GCP-BQ using pyspark(reformat the code to a single line if using windows):\n",
    "\n",
    "gcloud dataproc workflow-templates add-job pyspark \\\n",
    "    --step-id = job-load-dpr-bq \\\n",
    "    --jars= gs://jarspath\n",
    "    --properties=spark.app.name=\"Bigquery Loader -Daily Product Revenue\",spark.submit.deployMode=cluster,spark.yarn.appMasterEnv.DATE_URI=gs://, define other environment variables \\\n",
    "    --workflow-template= template_name \\\n",
    "    --start-after = job-daily-product-revenue \\\n",
    "    gs://script_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n",
      "abretaildev  GCE                                                     STOPPED  us-central1-f\n"
     ]
    }
   ],
   "source": [
    "#To start the DP cluster use the below command:\n",
    "\n",
    "!gcloud dataproc clusters list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters start cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the DP WF template:\n",
    "\n",
    "!gcloud dataproc workflow-templates instantiate template_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove a job from WF templatE:\n",
    "\n",
    "!gcloud dataproc workflow-templates remove-job template_name --step-id = step-id   --quiet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
